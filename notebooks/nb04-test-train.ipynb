{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:15:44.415749Z",
     "start_time": "2020-06-15T05:15:44.302582Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T05:28:22.181027Z",
     "start_time": "2020-06-17T05:28:22.107899Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_, clip_grad_value_\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from htools import *\n",
    "from incendio.callbacks import *\n",
    "from incendio.core import *\n",
    "from incendio.optimizers import *\n",
    "from incendio.utils import *\n",
    "from stormlight import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:15:49.602367Z",
     "start_time": "2020-06-15T05:15:49.474696Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: /Users/hmamin/stormlight\n"
     ]
    }
   ],
   "source": [
    "cd_root()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:15:49.829864Z",
     "start_time": "2020-06-15T05:15:49.607621Z"
    }
   },
   "outputs": [],
   "source": [
    "bs = 4\n",
    "shuffle = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:16:01.110965Z",
     "start_time": "2020-06-15T05:15:49.833325Z"
    }
   },
   "outputs": [],
   "source": [
    "tok, model = load_pretrained_gpt2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:16:01.372130Z",
     "start_time": "2020-06-15T05:16:01.115149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from data/datasets/gpt2_lm_tokens_tiny.pkl.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1506, (24096,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = LMDataset.from_pickle('data/datasets/gpt2_lm_tokens_tiny.pkl', tok)\n",
    "dl = DataLoader(ds, bs, shuffle=shuffle)\n",
    "len(ds), ds.tokens.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with return_tuple=True, won't work with current incendio setup. Options:\n",
    "\n",
    "- Have dataset return (x, x). Less memory-efficient though. Doesn't solve the issue of how to pass kwargs into model (e.g. labels=x).\n",
    "- Write model wrapper that lets us pass in x and automatically calls GPT2(x, labels=x). May need to write a faux loss function in this case since gpt2 \"forward\" returns loss.\n",
    "- Update incendio loop to handle 1 item getitems, kwargs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T05:05:06.885696Z",
     "start_time": "2020-06-17T05:05:06.720367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note: can't separate transformer and LM head into 2 groups because weights are shared w/ input embeddings.\n",
    "class ModelWrapper(BaseModel):\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x, labels=x)[:2]\n",
    "    \n",
    "\n",
    "class ModularGPT2LM(BaseModel):\n",
    "    \"\"\"Assemble a GPT2 language model into an Incendio-ready model with layer\n",
    "    groups, unfreezing, etc.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        gpt2 = model.transformer\n",
    "        self.config = model.config\n",
    "        self.drop = gpt2.drop\n",
    "        self.lm_head = model.lm_head\n",
    "        # This just affects layer groups in the optimizer. Dropout doesn't\n",
    "        # need to be trained and lm_head's weights are the same as wte.\n",
    "        self.groups = nn.ModuleList([gpt2.wte, \n",
    "                                     gpt2.wpe,\n",
    "                                     gpt2.h, \n",
    "                                     gpt2.ln_f])\n",
    "        \n",
    "    def forward(self, x, use_cache=True, past=None):\n",
    "        \"\"\"Heavily based on the Huggingface implementation with minor \n",
    "        simplifications for unused options.\n",
    "        \"\"\"\n",
    "        if not past:\n",
    "            past_len, past = 0, [None] * len(self.groups[-2])\n",
    "        else:\n",
    "            past_len = past[0][0].shape[-2]\n",
    "        x_shape = x.shape\n",
    "        x_ids = x.view(-1, x_shape[-1])\n",
    "        pos_ids = torch.arange(past_len, x_shape[-1] + past_len, \n",
    "                               dtype=torch.long, device=x.device).unsqueeze(0)\n",
    "        \n",
    "        # Token embeddings and positional encodings.\n",
    "        hidden = self.groups[0](x_ids) + self.groups[1](pos_ids)\n",
    "        hidden = self.drop(hidden)\n",
    "        \n",
    "        # Layer Norm, Attention blocks, more Layer Norm.\n",
    "        presents = []\n",
    "        for block, prev in zip(self.groups[-2], past):\n",
    "            outputs = block(hidden, layer_past=prev, use_cache=use_cache)\n",
    "            hidden, present = outputs[:2]\n",
    "            if use_cache: presents.append(present)\n",
    "        hidden = self.groups[-1](hidden)\n",
    "        \n",
    "        # logits: shape (bs, seq_len, vocab_size)\n",
    "        # presents (12 tensors - keys and values from attention blocks):\n",
    "        #     shape: (2, bs, num_heads, sequence_length, embed_size_per_head)\n",
    "        logits = self.lm_head(hidden)\n",
    "        return logits, tuple(presents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T05:05:24.985592Z",
     "start_time": "2020-06-17T05:05:24.668111Z"
    }
   },
   "outputs": [],
   "source": [
    "mod = ModularGPT2LM(model)\n",
    "mod.eval()\n",
    "res = mod(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T05:05:29.073077Z",
     "start_time": "2020-06-17T05:05:28.957460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 50258]) torch.Size([4, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(8.0986, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_loss_wrapper(res[0], x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue: LMhead model has use_cache=True by default, but in examples past=None so are we actually using this somewhere? Also, when I pass in `past`, results change. This is consistent with GPT2LMHead behavior, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T05:05:41.126183Z",
     "start_time": "2020-06-17T05:05:40.801676Z"
    }
   },
   "outputs": [],
   "source": [
    "res2 = wrap(x, past=res[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T05:05:41.642183Z",
     "start_time": "2020-06-17T05:05:41.531685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 50258]) torch.Size([4, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(7.8249, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_loss_wrapper(res2[0], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T05:05:50.882093Z",
     "start_time": "2020-06-17T05:05:50.521361Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 50258]) torch.Size([4, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(8.0986, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "res1 = model(x)\n",
    "lm_loss_wrapper(res1[0], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T05:06:34.531323Z",
     "start_time": "2020-06-17T05:06:34.159137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 50258]) torch.Size([4, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(7.8249, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res3 = model(x, past=res1[-1])\n",
    "lm_loss_wrapper(res3[0], x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T05:05:56.856424Z",
     "start_time": "2020-06-17T05:05:56.794183Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Adam (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 0.001\n",
       "    lr: 0.003\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 1\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 0.001\n",
       "    lr: 0.003\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 2\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 0.001\n",
       "    lr: 0.006\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 3\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 0.001\n",
       "    lr: 0.009\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = variable_lr_optimizer(mod, [3e-3, 3e-3, 6e-3, 9e-3])\n",
    "opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:49:09.744690Z",
     "start_time": "2020-06-17T04:49:09.690337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 144, 2]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(group['params']) for group in opt.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T04:50:30.184407Z",
     "start_time": "2020-06-17T04:50:30.106411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'weight': torch.Size([768]), 'bias': torch.Size([768])}"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v.shape for k, v in wrap.groups[-1].named_parameters()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T05:05:16.196810Z",
     "start_time": "2020-06-16T05:05:16.145542Z"
    }
   },
   "outputs": [],
   "source": [
    "wrap_mod = ModelWrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:16:35.330325Z",
     "start_time": "2020-06-15T05:16:35.251831Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 16])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = item(dl)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:16:36.047939Z",
     "start_time": "2020-06-15T05:16:35.991606Z"
    }
   },
   "outputs": [],
   "source": [
    "# Incendio validation method still won't work though.\n",
    "def lm_loss_wrapper(y_pred, y_true):\n",
    "#     print(y_pred)\n",
    "    print(y_pred.shape, y_true.shape)\n",
    "    # Discard last logit and first language model \"label\".\n",
    "    logits = y_pred[:, :-1, :].contiguous()\n",
    "    labels = y_true[:, 1:].flatten()\n",
    "    # shapes: (bs*(seq_len-1), vocab_size), (bs*(seq_len-1))\n",
    "    return F.cross_entropy(logits.view(-1, logits.shape[-1]), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_entropy                     y_shape: (bs,)    yhat_shape: (bs, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:16:50.111595Z",
     "start_time": "2020-06-15T05:16:48.345451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(wrap_mod.training)\n",
    "wrap_mod.eval()\n",
    "print(wrap_mod.training)\n",
    "with torch.no_grad():\n",
    "    res = wrap_mod(x)\n",
    "    \n",
    "print(wrap_mod.training)\n",
    "wrap_mod.train()\n",
    "print(wrap_mod.training)\n",
    "with torch.no_grad():\n",
    "    res2 = wrap_mod(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:16:51.099583Z",
     "start_time": "2020-06-15T05:16:51.044227Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:17:01.467185Z",
     "start_time": "2020-06-15T05:17:01.365567Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 50258]) torch.Size([4, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(95.8073), tensor(95.8073))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_loss_wrapper(res[1], x), res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:17:02.745831Z",
     "start_time": "2020-06-15T05:17:02.664079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 50258]) torch.Size([4, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(89.9666), tensor(89.9666))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_loss_wrapper(res2[1], x), res2[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guessing\n",
    "\n",
    "lm_head is excluded from named_parameters bc it's just pointing to wte. So a group consisting of [wte, wpe] will still include lm_head (I think)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T03:28:21.940099Z",
     "start_time": "2020-06-16T03:28:21.871424Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "emb = {k: v for k, v in model.named_parameters() \n",
    "       if any(term in k for term in ('wte', 'wpe'))}\n",
    "other = {k: v for k, v in model.named_parameters() if k not in emb.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T03:28:26.425230Z",
     "start_time": "2020-06-16T03:28:26.357535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 146)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb), len(other)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T06:04:31.743332Z",
     "start_time": "2020-06-15T06:04:31.677367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LayerNorm((768,), eps=1e-05, elementwise_affine=True)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.ln_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T06:04:13.805191Z",
     "start_time": "2020-06-15T06:04:13.690781Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=50258, bias=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.lm_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T05:35:25.805613Z",
     "start_time": "2020-06-17T05:35:25.720411Z"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerTrainer(Trainer):\n",
    "    \"\"\"Patched version of incendio.core.Trainer that works with Hugginface\n",
    "    Transformers. Eventually, Incendio's version should be flexible enough to\n",
    "    support them natively but for now it will be faster to do it this way.\n",
    "    \"\"\"\n",
    "    \n",
    "    @handle_interrupt\n",
    "    def fit(self, epochs, lrs=3e-3, lr_mult=1.0, **kwargs):\n",
    "        \"\"\"Train the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        epochs: int\n",
    "            Number of epochs to train for.\n",
    "        lrs: float or Iterable(float)\n",
    "            Pass in one or more learning rates. If lr_mult < 1, these\n",
    "            will be the max LR(s). If the number of values matches the number\n",
    "            of layer groups in the model, they will be matched accordingly,\n",
    "            with the first layer is assigned the first LR. If 1 LR is passed\n",
    "            in and lr_mult < 1, the multiplier will be used to create an\n",
    "            appropriate number of LRs. Example: for a network with 3 groups,\n",
    "            lrs=3e-3 and lr_mult=0.1 will produce LRs of [3e-5, 3e-4, 3e-3].\n",
    "        lr_mult: float\n",
    "            Multiplier used to compute additional learning rates if needed.\n",
    "            See `update_optimizer()` for details.\n",
    "        kwargs: any\n",
    "            Pass in clean=True to remove existing files in out_dir.\n",
    "        \"\"\"\n",
    "        stats = defaultdict(list)\n",
    "        sum_i = 0\n",
    "        _ = self.decide_stop('on_train_begin', epochs, lrs, lr_mult, **kwargs)\n",
    "        for e in range(epochs):\n",
    "            _ = self.decide_stop('on_epoch_begin', e, stats, None)\n",
    "            for i, xb in enumerate(self.pbar):\n",
    "                sum_i += 1\n",
    "                xb = xb.to(self.device)\n",
    "                self.optim.zero_grad()\n",
    "                _ = self.decide_stop('on_batch_begin', i, sum_i, stats)\n",
    "\n",
    "                # Forward and backward passes.\n",
    "                y_score, _ = self.net(xb)\n",
    "                loss = self.criterion(y_score, xb)\n",
    "                loss.backward()\n",
    "                if self.decide_stop('after_backward', e, i, sum_i, stats): break\n",
    "                self.optim.step()\n",
    "\n",
    "                # Separate because callbacks are only applied during training.\n",
    "                self._update_stats(stats, loss, xb[1:], y_score[:, :-1, :])\n",
    "                if self.decide_stop('on_batch_end', i, sum_i, stats): break\n",
    "\n",
    "            # If on_batch_end callback halts training, else block is skipped.\n",
    "            else:\n",
    "                val_stats = self.validate()\n",
    "                if self.decide_stop('on_epoch_end', e, stats, val_stats): break\n",
    "                continue\n",
    "            break\n",
    "        _ = self.decide_stop('on_train_end', e, stats, val_stats)\n",
    "        \n",
    "    def validate(self, dl_val=None):\n",
    "        \"\"\"Evaluate the model on a validation set.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dl_val: torch.utils.data.DataLoader\n",
    "            Accepting an optional dataloader allows the user to pass in\n",
    "            different loaders after training for evaluation. If None is\n",
    "            passed in, self.dl_val is used.\n",
    "        \"\"\"\n",
    "        dl_val = self.dl_val or dl_val\n",
    "        val_stats = defaultdict(list)\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb in tqdm(dl_val):\n",
    "                xb = xb.to(self.device)\n",
    "                loss, y_score = self.net(xb, labels=xb)\n",
    "                self._update_stats(val_stats, loss, xb[1:], y_score[:, :-1, :])\n",
    "        return val_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T05:35:27.037483Z",
     "start_time": "2020-06-17T05:35:26.984410Z"
    }
   },
   "outputs": [],
   "source": [
    "class GradientClipper(TorchCallback):\n",
    "    \n",
    "    @valuecheck\n",
    "    def __init__(self, mode:('norm', 'value')='norm', max_val=None):\n",
    "        self.clip_fn_ = getattr(nn.utils, f'clip_grad_{mode}_')\n",
    "        self.max_val = max_val or 1.0\n",
    "        \n",
    "    def after_backward(self, trainer, *args, **kwargs):\n",
    "        self.clip_fn_(trainer.model.parameters(), self.max_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T05:35:27.933053Z",
     "start_time": "2020-06-17T05:35:27.868524Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some default hypers from hugginface script.\n",
    "hypers = DotDict(lr=5e-5, max_grad_norm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T05:35:30.122614Z",
     "start_time": "2020-06-17T05:35:30.066749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer(criterion='lm_loss_wrapper', out_dir='data/models/v1')\n",
      "\n",
      "Datasets: 1506 train rows, 1506 val rows\n",
      "\n",
      "Optimizer: None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = TransformerTrainer(mod, ds, ds, dl, dl, lm_loss_wrapper, \n",
    "                       mode='multiclass', out_dir='data/models/v1')\n",
    "print(str(t)[:122])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T05:35:32.653117Z",
     "start_time": "2020-06-17T05:35:31.240037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-06-16 22:35:31,301 [INFO]: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 0.001\n",
      "    lr: 6.25e-06\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 0.001\n",
      "    lr: 1.25e-05\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 0.001\n",
      "    lr: 2.5e-05\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 0.001\n",
      "    lr: 5e-05\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb299249a3440cab2bd39edf670470a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=377.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16, 50258]) torch.Size([4, 16])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BasicConfig' object has no attribute 'after_backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-355-f247244d23f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/incendio/core.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0minstance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Stop training due to KeyboardInterrupt.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-351-c581b43e71d1>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lrs, lr_mult, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecide_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_backward'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msum_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/incendio/core.py\u001b[0m in \u001b[0;36mdecide_stop\u001b[0;34m(self, attr, *args, **kwargs)\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;31m# Pass model object as first argument to callbacks.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BasicConfig' object has no attribute 'after_backward'"
     ]
    }
   ],
   "source": [
    "out = t.fit(1, hypers.lr, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:11:32.262612Z",
     "start_time": "2020-06-15T05:11:32.202888Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T05:11:35.978642Z",
     "start_time": "2020-06-15T05:11:35.924246Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(104.2188, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T05:49:06.817937Z",
     "start_time": "2020-06-18T05:49:06.759260Z"
    }
   },
   "outputs": [],
   "source": [
    "edge = load_book('edge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T05:49:39.831167Z",
     "start_time": "2020-06-18T05:49:39.754182Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxbrxxPROLOGUE',\n",
       " 'Lift had never robbed a palace before. Seemed like a dangerous thing to try.',\n",
       " \"Not because she might get caught, but because once you robbed a starvin'\",\n",
       " 'palace, where did you go next?',\n",
       " 'She climbed up onto the outer wall and looked in at the grounds.']"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = [line for line in edge.splitlines()[:50] if line]\n",
    "lines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T05:50:07.015499Z",
     "start_time": "2020-06-18T05:50:06.930666Z"
    }
   },
   "outputs": [],
   "source": [
    "res = tok.batch_encode_plus(lines, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T05:50:33.376691Z",
     "start_time": "2020-06-18T05:50:33.301082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(x) for x in res['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T05:51:21.778192Z",
     "start_time": "2020-06-18T05:51:21.709018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"Grounds at this end look empty, as my informant indicated would be the',\n",
       " 'case,\" Huqin said. He was in charge of the lot of them. Had a nose like',\n",
       " 'someone had taken hold of it when he was a kid and pulled real, real hard.',\n",
       " \"Lift was surprised he didn't smack people in the face with it when he turned.\",\n",
       " '\"Everyone\\'s focused on choosing the new Prime Aqasix,\" said Maxin.',\n",
       " '\"We could really do this. Rob the Bronze Palace itself, and right under the',\n",
       " 'nose of the vizierate.\"',\n",
       " '\"Is it ... um ... safe?\" asked Huqin\\'s nephew. He was in his teens, and',\n",
       " \"puberty hadn't been kind to him. Not with that face, that voice, and those\",\n",
       " 'spindly legs.']"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[25:35]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
